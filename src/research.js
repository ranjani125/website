export default [
    {
        title: "Role of Lipschitz constant in Gradient Learning",
        abstract: "The pre-print demonstrates a way to initialize learning rate in gradient descent by exploiting functional properties of the loss function.",
        authors: "Snehanshu Saha and Rahul Yedida",
        year: 2019,
        link: "https://www.researchgate.net/publication/330521729_Role_of_Lipschitz_constant_in_Gradient_Learning"
    },
    {
        title: "\"How Prolific Are You?\": A Scientometric Analysis of Researchers",
        abstract: "Scholars in academia regularly write papers showing the results of their work. To gauge how relevant their work is, several measures are used, including citation count and the h-index. This paper describes the methods used to identify \"prolific\" authors, whose metrics are significantly higher than their peers, and provides a general discussion of the relationships between different metrics. We also discuss how this analysis is performed even with the high dimensionality of the dataset.",
        authors: "Rahul Yedida, Adel Aladwani, Snehanshu Saha",
        year: 2018,
        link: "https://www.researchgate.net/publication/329999539_How_Prolific_Are_You_A_Scientometric_Analysis_of_Researchers"
    },
    {
        title: "Optimizing Inter-nationality of Journals: A classical gradient approach revisited via Swarm Intelligence",
        abstract: "Particle Swarm Optimization (PSO) is an evolutionary algorithm that makes use of randomly initialized candidate solutions (known as particles) in an n-dimensional search space to optimize (minimize or maximize) a specified objective function. The algorithm does not make any assumption about the objective being optimized and does not calculate gradient of the function. PSO makes use of simple mathematical operators to enable the swarm to explore the search space for potential optima without bearing the overhead of calculating first order derivatives of the function in the fitness landscape. However, empirical observations reveal that the swarm traverses the search space in a direction that converges with the decreasing gradient (in minimization) of the objective function over many iterations. The paper proposes new insights to how the computed velocity of the particles in the swarm simulates gradient calculation.",
        authors: "Luckyson Khaidem, Rahul Yedida, Abhijit J Theophilus",
        year: 2019,
        link: "",
    }
];